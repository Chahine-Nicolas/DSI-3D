{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ec864ea-79c7-4172-8c48-f4c39b2b0816",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "18\n",
      "28\n",
      "38\n",
      "48\n",
      "58\n",
      "68\n",
      "78\n",
      "88\n",
      "98\n",
      "108\n",
      "118\n",
      "128\n",
      "138\n",
      "148\n",
      "158\n",
      "168\n",
      "178\n",
      "188\n",
      "198\n",
      "208\n",
      "218\n",
      "228\n",
      "238\n",
      "248\n",
      "258\n",
      "268\n",
      "278\n",
      "288\n",
      "298\n",
      "308\n",
      "318\n",
      "328\n",
      "338\n",
      "348\n",
      "358\n",
      "368\n",
      "378\n",
      "388\n",
      "398\n",
      "408\n",
      "418\n",
      "428\n",
      "438\n",
      "448\n",
      "458\n",
      "468\n",
      "478\n",
      "488\n",
      "498\n",
      "508\n",
      "518\n",
      "528\n",
      "538\n",
      "548\n",
      "558\n",
      "568\n",
      "578\n",
      "588\n",
      "598\n",
      "608\n",
      "618\n",
      "628\n",
      "638\n",
      "648\n",
      "658\n",
      "668\n",
      "678\n",
      "688\n",
      "698\n",
      "708\n",
      "718\n",
      "728\n",
      "738\n",
      "748\n",
      "758\n",
      "768\n",
      "778\n",
      "788\n",
      "798\n",
      "808\n",
      "818\n",
      "828\n",
      "838\n",
      "848\n",
      "858\n",
      "868\n",
      "878\n",
      "888\n",
      "898\n",
      "908\n",
      "918\n",
      "928\n",
      "938\n",
      "948\n",
      "958\n",
      "968\n",
      "978\n",
      "988\n",
      "998\n",
      "1008\n",
      "1018\n",
      "1028\n",
      "1038\n",
      "1048\n",
      "1058\n",
      "1068\n",
      "1078\n",
      "1088\n",
      "1098\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_queries = 18236\n",
    "len_eval = 0\n",
    "for query_idx in range(num_queries):\n",
    "    #for query_idx, input_data  in enumerate(eval_loader):\n",
    "    if query_idx > 13063 or query_idx < 11962:\n",
    "        continue\n",
    "    if query_idx % 10 != 0:\n",
    "        continue  \n",
    "    len_eval += 1\n",
    "    print(query_idx-11962)\n",
    "len_eval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d66708fe-a54a-4f9a-99fb-3965b7a28cc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "350\n",
      "360\n",
      "370\n",
      "380\n",
      "390\n",
      "400\n",
      "410\n",
      "420\n",
      "430\n",
      "440\n",
      "450\n",
      "460\n",
      "470\n",
      "480\n",
      "490\n",
      "500\n",
      "510\n",
      "520\n",
      "530\n",
      "540\n",
      "550\n",
      "560\n",
      "570\n",
      "580\n",
      "590\n",
      "600\n",
      "610\n",
      "620\n",
      "630\n",
      "640\n",
      "650\n",
      "660\n",
      "670\n",
      "680\n",
      "690\n",
      "700\n",
      "710\n",
      "720\n",
      "730\n",
      "740\n",
      "750\n",
      "760\n",
      "770\n",
      "780\n",
      "790\n",
      "800\n",
      "810\n",
      "820\n",
      "830\n",
      "840\n",
      "850\n",
      "860\n",
      "870\n",
      "880\n",
      "890\n",
      "900\n",
      "910\n",
      "920\n",
      "930\n",
      "940\n",
      "950\n",
      "960\n",
      "970\n",
      "980\n",
      "990\n",
      "1000\n",
      "1010\n",
      "1020\n",
      "1030\n",
      "1040\n",
      "1050\n",
      "1060\n",
      "1070\n",
      "1080\n",
      "1090\n",
      "1100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_queries = 1101\n",
    "len_eval = 0\n",
    "for query_idx in range(num_queries):\n",
    "    #for query_idx, input_data  in enumerate(eval_loader):\n",
    "    if query_idx % 10 != 0:\n",
    "        continue  \n",
    "    len_eval += 1\n",
    "    print(query_idx)\n",
    "len_eval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1ba5b01-a70d-419e-99cd-74ed21a207b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03eec6dc-260a-440e-8e6e-9c41387c4765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=256, out_features=4, bias=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = 256\n",
    "num_experts = 4\n",
    "\n",
    "torch.nn.Linear(input_dim, num_experts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56c56224-e737-4c98-82c1-6f5d6abb6eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ContextAwareGate(nn.Module):\n",
    "    def __init__(self, input_dim, num_experts):\n",
    "        super(ContextAwareGate, self).__init__()\n",
    "        self.gate = nn.Linear(input_dim * 2, num_experts)  # Takes query + place embedding\n",
    "\n",
    "    def forward(self, query, place_embedding):\n",
    "        combined_input = torch.cat([query, place_embedding], dim=-1)  # Shape: (batch, 512)\n",
    "        logits = self.gate(combined_input)  # Compute routing scores\n",
    "        gate_weights = F.softmax(logits, dim=-1)  # Normalize expert weights\n",
    "        return gate_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9707d161-8364-44cf-95d2-f980654f61ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextAwareMoE(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_experts=4):\n",
    "        super(ContextAwareMoE, self).__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.experts = nn.ModuleList([Expert(input_dim, output_dim) for _ in range(num_experts)])\n",
    "        self.gate = ContextAwareGate(input_dim, num_experts)  # Uses query + place info\n",
    "\n",
    "    def forward(self, query, place_embedding):\n",
    "        gate_weights = self.gate(query, place_embedding)  # Compute expert weights\n",
    "        expert_outputs = torch.stack([expert(query) for expert in self.experts], dim=-1)  # Shape: (batch, output_dim, num_experts)\n",
    "\n",
    "        # Compute final output as weighted sum of expert outputs\n",
    "        output = torch.sum(expert_outputs * gate_weights.unsqueeze(1), dim=-1)  # Shape: (batch, output_dim)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8ce35b-72e2-475d-8e3a-a47dd39e1065",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16c8b07-8894-4fa3-b761-cc0d00b830dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95ada3c8-8cc2-4370-aa34-7017f49477e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Expert Model\n",
    "class Expert(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Expert, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Context-Aware Gating Network\n",
    "class ContextAwareGate(nn.Module):\n",
    "    def __init__(self, input_dim, num_experts):\n",
    "        super(ContextAwareGate, self).__init__()\n",
    "        self.gate = nn.Linear(input_dim * 2, num_experts)  # Takes query + place embedding\n",
    "\n",
    "    def forward(self, query, place_embedding):\n",
    "        combined_input = torch.cat([query, place_embedding], dim=-1)  # Shape: (batch, 512)\n",
    "        logits = self.gate(combined_input)  \n",
    "        gate_weights = F.softmax(logits, dim=-1)  # Normalize expert weights\n",
    "        return gate_weights\n",
    "\n",
    "# MoE Model\n",
    "class ContextAwareMoE(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_experts=4):\n",
    "        super(ContextAwareMoE, self).__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.experts = nn.ModuleList([Expert(input_dim, output_dim) for _ in range(num_experts)])\n",
    "        self.gate = ContextAwareGate(input_dim, num_experts)\n",
    "\n",
    "    def forward(self, query, place_embedding):\n",
    "        gate_weights = self.gate(query, place_embedding)  \n",
    "        expert_outputs = torch.stack([expert(query) for expert in self.experts], dim=-1)  \n",
    "        output = torch.sum(expert_outputs * gate_weights.unsqueeze(1), dim=-1)  \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a279d8ab-3459-436d-87c4-686a5973effc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data(num_samples=1000, input_dim=256, output_dim=128, num_experts=4):\n",
    "    queries = torch.randn(num_samples, input_dim)  # (1000, 256)\n",
    "    place_embeddings = torch.randn(num_samples, input_dim)  # (1000, 256)\n",
    "\n",
    "    # Fix: Each expert has its own transformation matrix (input_dim, output_dim)\n",
    "    true_expert_weights = torch.randn(input_dim, output_dim)  \n",
    "\n",
    "    # Fix: Matrix multiplication is now valid\n",
    "    targets = torch.matmul(queries, true_expert_weights) + torch.matmul(place_embeddings, true_expert_weights)\n",
    "\n",
    "    return queries, place_embeddings, targets\n",
    "# Generate synthetic data before training\n",
    "queries, place_embeddings, targets = generate_synthetic_data()\n",
    "\n",
    "# Now, queries is properly defined before training starts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92c875af-625f-452b-8de0-bc4e4d9a4f97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9.0845e-01, -3.6132e-01,  1.6721e-02,  ..., -1.7323e+00,\n",
       "          1.2088e-03,  5.8697e-01],\n",
       "        [ 4.3316e-01, -1.1729e+00,  8.1999e-01,  ...,  1.6802e+00,\n",
       "         -8.3640e-01,  3.6989e-01],\n",
       "        [-8.8561e-01,  3.7932e-02, -2.2373e+00,  ...,  1.6334e-01,\n",
       "         -7.5929e-01,  5.3799e-01],\n",
       "        ...,\n",
       "        [ 4.8420e-01,  4.9542e-01,  1.5490e-01,  ..., -4.5567e-01,\n",
       "         -3.6636e-01,  6.1772e-01],\n",
       "        [-8.5167e-01, -3.8461e-01,  7.9173e-01,  ...,  1.1728e+00,\n",
       "          2.9940e-01,  3.5466e-01],\n",
       "        [-4.6029e-01,  3.6763e-01, -2.3279e-01,  ...,  4.9877e-01,\n",
       "          1.0159e+00,  2.7989e-01]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d70bcafd-c976-48a3-8549-c37d3beacd4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 16.297159271240233\n",
      "Epoch 2/50, Loss: 16.097980163574217\n",
      "Epoch 3/50, Loss: 15.91179067993164\n",
      "Epoch 4/50, Loss: 15.715748779296876\n",
      "Epoch 5/50, Loss: 15.511205291748047\n",
      "Epoch 6/50, Loss: 15.301211669921875\n",
      "Epoch 7/50, Loss: 15.089698883056641\n",
      "Epoch 8/50, Loss: 14.879258728027343\n",
      "Epoch 9/50, Loss: 14.671440521240234\n",
      "Epoch 10/50, Loss: 14.467148162841797\n",
      "Epoch 11/50, Loss: 14.266860046386718\n",
      "Epoch 12/50, Loss: 14.070824798583985\n",
      "Epoch 13/50, Loss: 13.879171325683593\n",
      "Epoch 14/50, Loss: 13.691947174072265\n",
      "Epoch 15/50, Loss: 13.509131256103515\n",
      "Epoch 16/50, Loss: 13.33064956665039\n",
      "Epoch 17/50, Loss: 13.156391571044923\n",
      "Epoch 18/50, Loss: 12.986221862792968\n",
      "Epoch 19/50, Loss: 12.81999530029297\n",
      "Epoch 20/50, Loss: 12.657565338134766\n",
      "Epoch 21/50, Loss: 12.498795486450195\n",
      "Epoch 22/50, Loss: 12.34355746459961\n",
      "Epoch 23/50, Loss: 12.1917294921875\n",
      "Epoch 24/50, Loss: 12.04319206237793\n",
      "Epoch 25/50, Loss: 11.897828186035156\n",
      "Epoch 26/50, Loss: 11.75552473449707\n",
      "Epoch 27/50, Loss: 11.616175048828126\n",
      "Epoch 28/50, Loss: 11.479677856445312\n",
      "Epoch 29/50, Loss: 11.345939270019532\n",
      "Epoch 30/50, Loss: 11.214871109008788\n",
      "Epoch 31/50, Loss: 11.086390808105469\n",
      "Epoch 32/50, Loss: 10.960419479370117\n",
      "Epoch 33/50, Loss: 10.836882217407226\n",
      "Epoch 34/50, Loss: 10.715707641601563\n",
      "Epoch 35/50, Loss: 10.596827362060546\n",
      "Epoch 36/50, Loss: 10.480174957275391\n",
      "Epoch 37/50, Loss: 10.365686569213867\n",
      "Epoch 38/50, Loss: 10.253301239013672\n",
      "Epoch 39/50, Loss: 10.142960006713867\n",
      "Epoch 40/50, Loss: 10.034606048583985\n",
      "Epoch 41/50, Loss: 9.928185607910157\n",
      "Epoch 42/50, Loss: 9.823646392822265\n",
      "Epoch 43/50, Loss: 9.720938774108888\n",
      "Epoch 44/50, Loss: 9.62001512145996\n",
      "Epoch 45/50, Loss: 9.520829864501954\n",
      "Epoch 46/50, Loss: 9.423338569641114\n",
      "Epoch 47/50, Loss: 9.327499328613282\n",
      "Epoch 48/50, Loss: 9.233271713256835\n",
      "Epoch 49/50, Loss: 9.140616882324219\n",
      "Epoch 50/50, Loss: 9.049497535705566\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define model, loss, and optimizer\n",
    "input_dim = 256\n",
    "output_dim = 128\n",
    "num_experts = 4\n",
    "learning_rate = 0.001\n",
    "num_epochs = 50\n",
    "batch_size = 32\n",
    "\n",
    "model = ContextAwareMoE(input_dim, output_dim, num_experts)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for i in range(0, len(queries), batch_size):\n",
    "        query_batch = queries[i:i+batch_size]\n",
    "        place_batch = place_embeddings[i:i+batch_size]\n",
    "        target_batch = targets[i:i+batch_size]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(query_batch, place_batch)\n",
    "        loss = loss_fn(output, target_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(queries)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f900541-3020-44fb-bc69-17e66da0f60f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sandbox",
   "language": "python",
   "name": "sandbox"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
