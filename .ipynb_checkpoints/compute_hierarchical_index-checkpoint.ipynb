{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02c4c2a1-2e25-42e7-9a78-0e9d31b98927",
   "metadata": {},
   "source": [
    "# save hierarchical indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "683a445d-436e-4d3f-91bc-4a8293bb2574",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import logging\n",
    "import json\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from module_loader_kitti_pose import load_poses_from_txt\n",
    "\n",
    "def elementwise_str_concat(prefix_list, suffix_list):\n",
    "    return [str(prefix) + str(suffix) for prefix, suffix in zip(prefix_list, suffix_list)]\n",
    "\n",
    "def reorder_to_original(J, X, clusters):\n",
    "    # Créer une liste d'indices pour trier `J` selon l'ordre original des documents\n",
    "    original_order = np.concatenate(clusters).argsort()\n",
    "    # Réordonner `J` en utilisant cet ordre original\n",
    "    return [J[i] for i in original_order]\n",
    "\n",
    "# verification duplicate\n",
    "def allDifferent1D(lst):\n",
    "    return len(lst) == len(set(lst))\n",
    "\n",
    "def generate_semantic_ids(X, c=10):\n",
    "    # Effectuer le clustering des documents en k=10 clusters et obtenir les labels directement\n",
    "    k = 10\n",
    "    \n",
    "    labels = KMeans(n_clusters=k).fit_predict(X)\n",
    "    \n",
    "    # Grouper les indices des documents par label\n",
    "    clusters = [np.where(labels == i)[0] for i in range(k)]\n",
    "    \n",
    "    J = []\n",
    "    for i in range(k):\n",
    "        current_cluster_size = len(clusters[i])\n",
    "        Jcurrent = [str(i)] * current_cluster_size\n",
    "        \n",
    "        if current_cluster_size > c:\n",
    "            # Appliquer la fonction récursive à ce cluster\n",
    "            sub_cluster_embeddings = X[clusters[i]] # sélectionne les descripteurs du cluster courant\n",
    "            Jrest = generate_semantic_ids(sub_cluster_embeddings, c) # génère les noms, récursif\n",
    "            \n",
    "        else:\n",
    "            # Générer des indices numériques pour les documents dans ce cluster\n",
    "            Jrest = list(map(str, range(current_cluster_size)))\n",
    "        \n",
    "        # Concaténer les préfixes et les suffixes pour obtenir les identifiants du cluster\n",
    "        Jcluster = elementwise_str_concat(Jcurrent, Jrest)\n",
    "        J.extend(Jcluster)\n",
    "    \n",
    "    # Réordonner les identifiants pour correspondre à l'ordre original des documents\n",
    "    J = reorder_to_original(J, X, clusters) # réordonne les ids hierarichique selon l'ordre des descripteurs (et non des clusters)\n",
    "\n",
    "    # verification duplicate\n",
    "    def allDifferent1D(lst):\n",
    "        return len(lst) == len(set(lst))\n",
    "    if allDifferent1D(J) == False:\n",
    "        print(\"Il y a des doublons !\")\n",
    "\n",
    "    return J\n",
    "\n",
    "def compute_hierarchical_clustering(eval_seq = 6, save = False):\n",
    "    log3dnet_dir=os.getenv('LOG3DNET_DIR')\n",
    "    ## ==== Kitti =====\n",
    "    print(\"kitti dataset\")\n",
    "    \n",
    "    kitti_dir = os.getenv('WORKSF') + \"/datas/datasets/\"\n",
    "    \n",
    "    eval_seq = '%02d' % eval_seq\n",
    "    sequence_path = kitti_dir + 'sequences/' + eval_seq + '/'\n",
    "    _, positions_database = load_poses_from_txt(\n",
    "            sequence_path + 'poses.txt')\n",
    "    num_queries =  len(positions_database)\n",
    "    embeddings = []\n",
    "    for query_idx in range(num_queries):\n",
    "        padded_string = str(query_idx).zfill(6)\n",
    "        print(padded_string)           \n",
    "        log_desc = sequence_path + \"/logg_desc/\" + padded_string + '.pt'\n",
    "    \n",
    "        xx = torch.load(log_desc)\n",
    "        embeddings.append(xx)\n",
    "\n",
    "    emb_cpu = torch.stack(embeddings).detach().cpu().numpy()\n",
    "    J = generate_semantic_ids(emb_cpu , c=10)\n",
    "    docid_map = {k: J[k] for k in range(len(J)) } # creat dict\n",
    "    for i in range(len(J)):\n",
    "        print(f\"Document {i}: Identifier {J[i]}\")\n",
    "\n",
    "    json_path = sequence_path + '/hierarchical.json'\n",
    "    # Print the generated identifiers\n",
    "    for doc_idx, doc_id in docid_map.items():\n",
    "        print(f\"Document {doc_idx}: Identifier {doc_id}\")\n",
    "    \n",
    "    \n",
    "    if save:\n",
    "        with open(json_path, \"w\") as json_file:\n",
    "            json.dump(docid_map, json_file)  \n",
    "        print(\"docid_map saved at \", json_path)\n",
    "compute_hierarchical_clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a7c817f-71ab-45c0-b33e-4118ee7e47ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'positions_database' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpositions_database\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'positions_database' is not defined"
     ]
    }
   ],
   "source": [
    "positions_database"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "loggDSI",
   "language": "python",
   "name": "loggdsi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
